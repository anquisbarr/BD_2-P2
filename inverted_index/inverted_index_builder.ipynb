{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/squispeb/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/squispeb/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns clean tokens from a tweet given\n",
    "def stopwords_stemmer(tweet_text):\n",
    "    tokens = nltk.word_tokenize(tweet_text)\n",
    "    stoplist = stopwords.words(\"spanish\")\n",
    "    stoplist += ['/…','--','RT','`','@','|','¿','?', '¡', '!', '.', ',', ';', '«', '»', ':', '(', ')', '\"','#', '$', '^', '&', '*', '%','IndianArmyPeoplesArmy']\n",
    "    tokens_clean = tokens.copy()\n",
    "\n",
    "    for token in tokens:\n",
    "        if token in stoplist:\n",
    "            tokens_clean.remove(token)\n",
    "            \n",
    "    stemmer = SnowballStemmer(\"spanish\")\n",
    "    for i in range(len(tokens_clean)):\n",
    "        tokens_clean[i] = stemmer.stem(tokens_clean[i])\n",
    "        \n",
    "    return tokens_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# json_path = './data/json_files/data_elecciones/'\n",
    "json_path = './data/json_files/ucl/'\n",
    "json_folder = os.listdir(json_path)\n",
    "\n",
    "# Dictionaries\n",
    "inverted_index = {}\n",
    "terms_dict = {}\n",
    "files_dict = {}\n",
    "tweets_dict = {}\n",
    "terms_df_dict = {}\n",
    "\n",
    "# Variables\n",
    "n_tweets = 0\n",
    "n_terms = 0\n",
    "\n",
    "# Building inverted index with every JSON file\n",
    "for json_file in json_folder:\n",
    "    file_id = len(files_dict)\n",
    "    files_dict[file_id] = json_file\n",
    "    with open(json_path + json_file,encoding=\"utf8\") as file:\n",
    "        json_content = json.load(file)\n",
    "        for tweet in json_content:\n",
    "            n_tweets += 1\n",
    "            clean_tokens = stopwords_stemmer(tweet[\"text\"])\n",
    "            term_freq = Counter(clean_tokens)\n",
    "            tweet_id = tweet[\"id\"]\n",
    "            norm = 0\n",
    "            tweets_dict[tweet_id] = [file_id,{},norm] #file_id,tf_idf,norm\n",
    "            for term in term_freq:\n",
    "                freq = term_freq[term]\n",
    "                \n",
    "                if not (term in terms_dict):\n",
    "                    terms_dict[term] = len(terms_dict)\n",
    "                \n",
    "                term_id = terms_dict[term]\n",
    "                tweets_dict[tweet_id][1][term_id]  = 0.0\n",
    "\n",
    "                if not (term_id in inverted_index):\n",
    "                    inverted_index[term_id] = {tweet_id : freq}\n",
    "                else:\n",
    "                    inverted_index[term_id][tweet_id] = freq\n",
    "\n",
    "for term_id in inverted_index:\n",
    "    tweet_list = inverted_index[term_id]\n",
    "    terms_df_dict[term_id] = len(tweet_list)\n",
    "\n",
    "n_terms = len(terms_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def getTF_IDF(tweet_list, tweet_id):\n",
    "    tf = tweet_list[tweet_id]\n",
    "    df = float(len(tweet_list))\n",
    "    return math.log(1+tf, 10) * math.log(n_tweets / df, 10)\n",
    "\n",
    "def setTF_IDF():\n",
    "    for term_id in inverted_index:\n",
    "        tweet_list = inverted_index[term_id]\n",
    "        for tweet_id in tweet_list:\n",
    "            tweets_dict[tweet_id][1][term_id] = getTF_IDF(tweet_list, tweet_id)\n",
    "\n",
    "def getNorm(tweet_list):\n",
    "    val = np.array(list(tweet_list.values()))\n",
    "    return np.linalg.norm(val)\n",
    "\n",
    "def setNorm():\n",
    "    for tweet_id in tweets_dict:\n",
    "        tweet_list = tweets_dict[tweet_id][1]\n",
    "        tweets_dict[tweet_id][2] = getNorm(tweet_list)\n",
    "\n",
    "# Setting TF_IDF and Norm for all terms\n",
    "setTF_IDF()\n",
    "setNorm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets:  25217 27090\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "bin_path = './data/bin/'\n",
    "\n",
    "#Files\n",
    "data = [inverted_index,terms_dict,files_dict,tweets_dict,terms_df_dict,n_tweets,n_terms]\n",
    "filenames = ['inverted_index', 'terms_dict', 'files_dict','tweets_dict','terms_df_dict','n_tweets','n_terms']\n",
    "i = 0\n",
    "\n",
    "# Data Serialization\n",
    "for d in data:\n",
    "    file = open (bin_path + filenames[i] + '.dat','wb+')\n",
    "    pickle.dump(d,file)\n",
    "    file.close()\n",
    "    i += 1\n",
    "\n",
    "print(\"Number of tweets: \",n_tweets,n_terms)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
